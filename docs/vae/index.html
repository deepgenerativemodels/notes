<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Variational Autoencoders</title>
  <meta name="description" content="Lecture notes for Deep Generative Models.">


  <link rel="stylesheet" href="/notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-129020129-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/notes/vae/">
  <link rel="alternate" type="application/rss+xml" title="Deep Generative Models" href="http://localhost:4000/notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/notes/">Contents</a>
	<a href="http://deepgenerativemodels.github.io">Class</a>
	<a href="http://github.com/deepgenerativemodels/notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Variational autoencoders</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<div class="mathblock"><script type="math/tex; mode=display">
\newcommand{\D}{\mathcal{D}}
\newcommand{\KL}[2]{D_\mathrm{KL}\paren{#1 \mathbin{\|} #2}}
\newcommand{\P}{\mathcal{P}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\M}{\mathcal{B}}
\newcommand{\ELBO}{\mathrm{ELBO}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\giv}{\mid}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\brac}[1]{\left[#1\right]}
\newcommand{\veps}{\varepsilon}
\newcommand{\set}[1]{\left\{#1\right\}}
\renewcommand{\d}{\mathop{}\!\mathrm{d}}
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\0}{\mathbf{0}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
</script></div>

<p>Latent variable models form a rich class of probabilistic models that can infer hidden structure in the underlying data. In this post, we will study variational autoencoders, which are a powerful class of deep generative models with latent variables.</p>

<h1 id="representation">Representation</h1>

<p>Consider a directed, latent variable model as shown below.</p>

<figure>
<img src="vae.png" alt="drawing" width="100" class="center" />
<figcaption>
Graphical model for a directed, latent variable model.
 </figcaption>
</figure>

<p>In the model above, <script type="math/tex">\bz</script> and <script type="math/tex">\bx</script> denote the latent and observed variables respectively. The joint distribution expressed by this model is given as</p>
<div class="mathblock"><script type="math/tex; mode=display">
p_\theta(\bx, \bz) = p(\bx \giv \bz)p(\bz).
</script></div>

<p>From a generative modeling perspective, this model describes a generative process for the observed data <script type="math/tex">\bx</script> using the following procedure</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\bz &\sim p(\bz) \\
\bx &\sim p(\bx \giv \bz).
\end{align}
</script></div>

<p>If one adopts the belief that the latent variables <script type="math/tex">\bz</script> somehow encode semantically meaningful information about <script type="math/tex">\bx</script>, it is natural to view this generative process as first generating the “high-level” semantic information about <script type="math/tex">\bx</script> first before fully generating <script type="math/tex">\bx</script>. Such a perspective motivates generative models with rich latent variable structures such as hierarchical generative models <script type="math/tex">p(\bx, \bz_1, \ldots, \bz_m) = p(\bx \giv \bz_1)\prod_i p(\bz_i \giv \bz_{i+1})</script>—where information about <script type="math/tex">\bx</script> is generated hierarchically—and temporal models such as the Hidden Markov Model—where temporally-related high-level information is generated first before constructing <script type="math/tex">\bx</script>.</p>

<p>We now consider a family of distributions <script type="math/tex">\P_\bz</script> where <script type="math/tex">p(\bz) \in \P_\bz</script> describes a probability distribution over <script type="math/tex">\bz</script>. Next, consider a family of conditional distributions <script type="math/tex">\P_{\bx\giv \bz}</script> where <script type="math/tex">p_\theta(\bx \giv \bz) \in \P_{\bx\giv \bz}</script> describes a conditional probability distribution over <script type="math/tex">\bx</script> given <script type="math/tex">\bz</script>. Then our hypothesis class of generative models is the set of all possible combinations</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\P_{\bx,\bz} = \set{p(\bx, \bz) \giv p(\bz) \in \P_\bz, p(\bx \giv \bz) \in \P_{\bx\giv\bz}}.
\end{align}
</script></div>
<p>Given a dataset <script type="math/tex">\D = \set{\bx^{(1)}, \ldots, \bx^{(n)}}</script>, we are interested in the following learning and inference tasks</p>
<ul>
  <li>Selecting <script type="math/tex">p \in \P_{\bx,\bz}</script> that “best” fits <script type="math/tex">\D</script>.</li>
  <li>Given a sample <script type="math/tex">\bx</script> and a model <script type="math/tex">p \in \P_{\bx,\bz}</script>, what is the posterior distribution over the latent variables <script type="math/tex">\bz</script>?
<!-- - Approximate marginal inference of $$\bx$$: given partial access to certain dimensions of the vector $$\bx$$, how do we impute the missing parts? --></li>
</ul>

<!-- We shall also assume the following
- Intractability: computing the posterior probability $$p(\bz \giv \bx)$$ is intractable.
- Big data: the dataset $$\D$$ is too large to fit in memory; we can only work with small, sub-sampled batches of $$\D$$. -->

<h1 id="learning-directed-latent-variable-models">Learning Directed Latent Variable Models</h1>

<p>One way to measure how closely <script type="math/tex">p(\bx, \bz)</script> fits the observed dataset <script type="math/tex">\D</script> is to measure the Kullback-Leibler (KL) divergence between the data distribution (which we denote as <script type="math/tex">p_{\mathrm{data}}(\bx)</script>) and the model’s marginal distribution <script type="math/tex">p(\bx) = \int p(\bx, \bz) \d \bz</script>. The distribution that ``best’’ fits the data is thus obtained by minimizing the KL divergence.</p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\min_{p \in \P_{\bx, \bz}}\KL{p_{\mathrm{data}}(\bx)}{p(\bx)}.
\end{align}
</script></div>

<p>As we have seen previously, optimizing an empirical estimate of the KL divergence is equivalent to maximizing the marginal log-likelihood <script type="math/tex">\log p(\bx)</script> over <script type="math/tex">\D</script></p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\max_{p \in \P_{\bx, \bz}} \sum_{\bx \in \D} \log p(\bx) = \sum_{\bx \in \D} \log\int p(\bx, \bz) \d \bz.
\end{align}
</script></div>

<p>However, it turns out this problem is generally intractable for high-dimensional <script type="math/tex">\bz</script> as it involves an integration (or sums in the case <script type="math/tex">\bz</script> is discrete) over all the possible latent sources of variation <script type="math/tex">\bz</script>. One option is to estimate the objective via Monte Carlo. For any given datapoint <script type="math/tex">\bf x</script>, we can obtain the following estimate for its marginal log-likelihood</p>

<div class="mathblock"><script type="math/tex; mode=display">
\log p(\bx) \approx \log \frac{1}{k} \sum_{i=1}^k p(\bx \vert \bz^{(i)}) \text{, where } \bz^{(i)} \sim p(\bz)
</script></div>

<p>In practice however, optimizing the above estimate suffers from high variance in gradient estimates.</p>

<p>Rather than maximizing the log-likelihood directly, an alternate is to instead construct a lower bound that is more amenable to optimization. To do so, we note that evaluating the marginal likelihood <script type="math/tex">p(\bx)</script> is at least as difficult as as evaluating the posterior <script type="math/tex">p(\bz \mid \bx)</script> for any latent vector <script type="math/tex">\bz</script> since by definition <script type="math/tex">p(\bz \mid \bx) = p(\bx, \bz) / p(\bx)</script>.</p>

<p>Next, we introduce a variational family <script type="math/tex">\Q</script> of distributions that approximate the true, but intractable posterior <script type="math/tex">p(\bz \mid \bx)</script>. Further henceforth, we will assume a parameteric setting where any distribution in the model family <script type="math/tex">\P_{\bx, \bz}</script> is specified via a set of parameters <script type="math/tex">\theta \in \Theta</script> and distributions in the variational family <script type="math/tex">\Q</script> are specified via a set of parameters <script type="math/tex">\lambda \in \Lambda</script>.</p>

<p>Given <script type="math/tex">\P_{\bx, \bz}</script> and <script type="math/tex">\Q</script>, we note that the following relationships hold true<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> for any <script type="math/tex">\bx</script> and all variational distributions <script type="math/tex">q_\lambda(\bz) \in \Q</script></p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\log p_\theta(\bx) &= \log \int p_\theta(\bx, \bz) \d \bz \\
&= \log \int \frac{q_\lambda(\bz)}{q_\lambda(\bz)} p(\bx, \bz) \d \bz\\
&\ge\int q_\lambda(\bz) \log \frac{p_\theta(\bx, \bz)}{q_\lambda(\bz)} \d \bz \\
&= \Expect_{q_\lambda(\bz)} \left[\log \frac{p_\theta(\bx, \bz)}{q_\lambda(\bz)}\right] \\
&:=\ELBO(\bx; \theta, \lambda)
\end{align}
</script></div>
<p>where we have used Jensen’s inequality in the final step. The Evidence Lower Bound or ELBO in short admits a tractable unbiased Monte Carlo estimator</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\frac{1}{k} \sum_{i=1}^k \log \frac{p_\theta(\bx, \bz^{(i)})}{q_\lambda(\bz^{(i)})} \text{, where } \bz^{(i)} \sim q_\lambda(\bz),
\end{align}
</script></div>
<p>so long as it is easy to sample from and evaluate densities for <script type="math/tex">q_\lambda(\bz)</script>.</p>

<p>Which variational distribution should we pick? Even though the above derivation holds for any choice of variational parameters <script type="math/tex">\lambda</script>, the tightness of the lower bound depends on the specific choice of <script type="math/tex">q</script>.</p>

<figure>
<img src="klgap.png" alt="drawing" width="400" class="center" />
<figcaption>
Illustration for the KL divergence gap between the marginal log-likelihood \(\log p_\theta(\bx)\) for a point \(\bx\) and the corresponding ELBO for a single 1D-parameter variational distribution \(q_\lambda(\bx)\).
 </figcaption>
</figure>

<p>In particular, the gap between the original objective(marginal log-likelihood <script type="math/tex">\log p_\theta(\bx)</script>) and the ELBO equals the KL divergence between the approximate posterior <script type="math/tex">q(\bz)</script> and the true posterior <script type="math/tex">p(\bz \giv \bx)</script>. The gap is zero when the variational distribution <script type="math/tex">q_\lambda(\bz)</script> exactly matches <script type="math/tex">p_\theta(\bz \giv \bx)</script>.</p>

<p>In summary, we can learn a latent variable model by maximizing the ELBO with respect to both the model parameters <script type="math/tex">\theta</script> and the variational parameters <script type="math/tex">\lambda</script> for any given datapoint <script type="math/tex">\bx</script></p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\max_{\theta} \sum_{\bx \in \D} \max_{\lambda} \Expect_{q_\lambda(\bz)} \left[\log \frac{p_\theta(\bx, \bz)}{q_\lambda(\bz)}\right].
\end{align}
</script></div>

<h1 id="black-box-variational-inference">Black-Box Variational Inference</h1>

<p>In this post, we shall focus on first-order stochastic gradient methods for optimizing the ELBO. These optimization techniques are desirable in that they allow us to sub-sample the dataset during optimization—but require our objective function to be differentiable with respect to the optimization variables. 
<!-- As such, we shall posit for now that any $$p(\bx, \bz) \in \P_{\bx, \bz}$$ and $$q(\bz) \in \Q$$ are alternatively parameterizable as $$p_\theta(\bx, \bz)$$ and $$q_\lambda(\bz)$$ and that these distributions are differentiable with respect to $$\theta$$ and $$\lambda$$. -->
This inspires Black-Box Variational Inference (BBVI), a general-purpose Expectation-Maximization-like algorithm for variational learning of latent variable models, where, for each mini-batch <script type="math/tex">\M = \set{\bx^{(1)}, \ldots, \bx^{(m)}}</script>, the following two steps are performed.</p>

<p><strong>Step 1</strong></p>

<p>We first do <em>per-sample</em> optimization of <script type="math/tex">q</script> by iteratively applying the update</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\lambda^{(i)} \gets \lambda^{(i)} + \tilde{\nabla}_\lambda \ELBO(\bx^{(i)}; \theta, \lambda^{(i)}),
\end{align}
</script></div>
<p>where <script type="math/tex">\text{ELBO}(\bx; \theta, \lambda) = \Expect_{q_\lambda(\bz)} \left[\log \frac{p_\theta(\bx, \bz)}{q_\lambda(\bz)}\right]</script>, and <script type="math/tex">\tilde{\nabla}_\lambda</script> denotes an unbiased estimate of the ELBO gradient. This step seeks to approximate the log-likelihood <script type="math/tex">\log p_\theta(\bx^{(i)})</script>.</p>

<p><strong>Step 2</strong></p>

<p>We then perform a single update step based on the mini-batch</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\theta \gets \theta + \tilde{\nabla}_\theta \sum_{i} \ELBO(\bx^{(i)}; \theta, \lambda^{(i)}),
\end{align}
</script></div>
<p>which corresponds to the step that hopefully moves <script type="math/tex">p_\theta</script> closer to <script type="math/tex">p_{\mathrm{data}}</script>.</p>

<h1 id="gradient-estimation">Gradient Estimation</h1>

<p>The gradients <script type="math/tex">\nabla_\lambda \ELBO</script> and <script type="math/tex">\nabla_\theta \ELBO</script> can be estimated via Monte Carlo sampling. While it is straightforward to construct an unbiased estimate of <script type="math/tex">\nabla_\theta \ELBO</script> by simply pushing <script type="math/tex">\nabla_\theta</script> through the expectation operator, the same cannot be said for <script type="math/tex">\nabla_\lambda</script>. Instead, we see that</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\nabla_\lambda \Expect_{q_\lambda(\bz)} \left[\log \frac{p_\theta(\bx, \bz)}{q_\lambda(\bz)} \right]= \Expect_{q_\lambda(\bz)} \brac{\paren{\log \frac{p_\theta(\bx, \bz)}{q_\lambda(\bz)}} \cdot \nabla_\lambda \log q_\lambda(\bz)}.
\end{align}
</script></div>
<p>This equality follows from the log-derivative trick (also commonly referred to as the REINFORCE trick). The full derivation involves some simple algebraic manipulations and is left as an exercise for the reader. The gradient estimator <script type="math/tex">\tilde{\nabla}_\lambda \ELBO</script> is thus</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\frac{1}{k}\sum_{i=1}^k \brac{\paren{\log \frac{p_\theta(\bx, \bz^{(i)})}{q_\lambda(\bz^{(i)})}} \cdot \nabla_\lambda \log q_\lambda(\bz^{(i)})} \text{, where } \bz^{(i)} \sim q_\lambda(\bz).
\end{align}
</script></div>
<p>However, it is often noted that this estimator suffers from high variance. One of the key contributions of the variational autoencoder paper is the reparameterization trick, which introduces a fixed, auxiliary distribution <script type="math/tex">p(\veps)</script> and a differentiable function <script type="math/tex">T(\veps; \lambda)</script> such that the procedure</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\veps &\sim p(\veps)\\
\bz &\gets T(\veps; \lambda),
\end{align}
</script></div>
<p>is equivalent to sampling from <script type="math/tex">q_\lambda(\bz)</script>. By the <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">Law of the Unconscious Statistician</a>, we can see that</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\nabla_\lambda \Expect_{q_\lambda(\bz)} \left[\log \frac{p_\theta(\bx, \bz)}{q_\lambda(\bz)}\right] = \Expect_{p(\veps)} \left[\nabla_\lambda \log \frac{p_\theta(\bx, T(\veps; \lambda))}{q_\lambda(T(\veps; \lambda))}\right].
\end{align}
</script></div>
<p>In contrast to the REINFORCE trick, the reparameterization trick is often noted empirically to have lower variance and thus results in more stable training. 
<!-- \rs{I think there exists pathological examples where REINFORCE has lower variance than reparamterization. Should we talk about that?} --></p>

<h1 id="parameterizing-distributions-via-deep-neural-networks">Parameterizing Distributions via Deep Neural Networks</h1>

<p>So far, we have described <script type="math/tex">p_\theta(\bx, \bz)</script> and <script type="math/tex">q_\lambda(\bz)</script> in the abstract. To instantiate these objects, we consider choices of parametric distributions for <script type="math/tex">p_\theta(\bz)</script>, <script type="math/tex">p_\theta(\bx \giv \bz)</script>, and <script type="math/tex">q_\lambda(\bz)</script>. A popular choice for <script type="math/tex">p_\theta(\bz)</script> is the unit Gaussian</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
p_\theta(\bz) = \Normal(\bz \giv \0, \I).
\end{align}
</script></div>
<p>in which case <script type="math/tex">\theta</script> is simply the empty set since the prior is a fixed distribution. Another alternative often used in practice is a mixture of Gaussians with trainable mean and covariance parameters.</p>

<p>The conditional distribution <script type="math/tex">p_\theta(\bx \giv \bz)</script> is where we introduce a deep neural network. We note that a conditional distribution can be constructed by defining a distribution family (parameterized by <script type="math/tex">\omega \in \Omega</script>) in the target space <script type="math/tex">\bx</script> (i.e. <script type="math/tex">p_\omega(\bx)</script> defines an unconditional distribution over <script type="math/tex">\bx</script>) and a mapping function <script type="math/tex">g_\theta: \Z \to \Omega</script>. 
<!-- It is natural to call $$g_\theta$$ the decoder that is parameterized by $$\theta$$. The act of conditioning on $$\bz$$ is thus equivalent to using the choice of $$\omega = g(\bz)$$. --> 
In other words, <script type="math/tex">g_\theta(\cdot)</script> defines the conditional distribution</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    p_\theta(\bx \giv \bz) = p_\omega(\bx) \text{ , where } \omega = g_\theta(\bz).
\end{align}
</script></div>
<p>The function <script type="math/tex">g_\theta</script> is also referred to as the decoding distribution since it maps a latent <em>code</em> <script type="math/tex">\bz</script> to the parameters of a distribution over observed variables <script type="math/tex">\bx</script>. In practice, it is typical to specify <script type="math/tex">g_\theta</script> as a deep neural network.<br />
<!-- The generative model $$p_\theta(\bx, \bz)$$ is called a *deep* generative model since we will be using a neural network to instantiate the function $$g_\theta$$.  -->
In the case where <script type="math/tex">p_\theta(\bx \giv \bz)</script> is a Gaussian distribution, we can thus represent it as</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    p_\theta(\bx \giv \bz) = \Normal(\bx \giv \mu_\theta(\bz), \Sigma_\theta(\bz)),
\end{align}
</script></div>
<p>where <script type="math/tex">\mu_\theta(\bz)</script> and <script type="math/tex">\Sigma_\theta(\bz)</script> are neural networks that specify the mean and covariance matrix for the Gaussian distribution over <script type="math/tex">\bx</script> when conditioned on <script type="math/tex">\bz</script>.</p>

<p>Finally, the variational family for the proposal distribution <script type="math/tex">q_\lambda(\bz)</script> needs to be chosen judiciously so that the reparameterization trick is possible. Many continuous distributions in the <a href="https://en.wikipedia.org/wiki/Location%E2%80%93scale_family">location-scale family</a> can be reparameterized. In practice, a popular choice is again the Gaussian distribution, where</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    \lambda &= (\mu, \Sigma) \\
    q_\lambda(\bz) &= \Normal(\bz \giv \mu, \Sigma)\\
    p(\veps) &= \Normal(\bz \giv \0, \I) \\
    T(\veps; \lambda) &= \mu + \Sigma^{1/2}\veps,
\end{align}
</script></div>
<p>where <script type="math/tex">\Sigma^{1/2}</script> is the Cholesky decomposition of <script type="math/tex">\Sigma</script>. For simplicity, practitioners often restrict <script type="math/tex">\Sigma</script> to be a diagonal matrix (which restricts the distribution family to that of factorized Gaussians).</p>

<h1 id="amortized-variational-inference">Amortized Variational Inference</h1>

<p>A noticable limitation of black-box variational inference is that <strong>Step 1</strong> executes an optimization subroutine that is computationally expensive. Recall that the goal of the <strong>Step 1</strong> is to find</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    \lambda^* = \argmax_{\lambda\in \Lambda} \ELBO(\bx; \theta, \lambda).
\end{align}
</script></div>
<p>For a given choice of <script type="math/tex">\theta</script>, there is a well-defined mapping from <script type="math/tex">\bx \mapsto \lambda^\ast</script>. A key realization is that this mapping can be <em>learned</em>. In particular, one can train an encoding function (parameterized by <script type="math/tex">\phi</script>) <script type="math/tex">f_\phi: \X \to \Lambda</script> 
(where <script type="math/tex">\Lambda</script> is the space of <script type="math/tex">\lambda</script> parameters) 
on the following objective</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    \max_{\phi } \sum_{\bx \in \D} \ELBO(\bx; \theta, f_\phi(\bx)).
\end{align}
</script></div>
<p>It is worth noting at this point that <script type="math/tex">f_\phi(\bx)</script> can be interpreted as defining the conditional distribution <script type="math/tex">q_\phi(\bz \giv \bx)</script>. With a slight abuse of notation, we define</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    \ELBO(\bx; \theta, \phi) = \Expect_{q_\phi(\bz \mid \bx)} \left[\log \frac{p_\theta(\bx, \bz)}{q_\phi(\bz \giv \bx)}\right].
\end{align}
</script></div>
<p>and rewrite the optimization problem as</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    \max_{\phi } \sum_{\bx \in \D} \ELBO(\bx; \theta, \phi).
\end{align}
</script></div>
<p>It is also worth noting that optimizing <script type="math/tex">\phi</script> over the entire dataset as a <em>subroutine</em> everytime we sample a new mini-batch is clearly not reasonable. However, if we believe that <script type="math/tex">f_\phi</script> is capable of quickly adapting to a close-enough approximation of <script type="math/tex">\lambda^\ast</script> given the current choice of <script type="math/tex">\theta</script>, then we can interleave the optimization <script type="math/tex">\phi</script> and <script type="math/tex">\theta</script>. The yields the following procedure, where for each mini-batch <script type="math/tex">\M = \set{\bx^{(1)}, \ldots, \bx^{(m)}}</script>, we perform the following two updates jointly</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
    \phi &\gets \phi + \tilde{\nabla}_\phi \sum_{\bx \in \M} \ELBO(\bx; \theta, \phi) \\
    \theta &\gets \theta + \tilde{\nabla}_\theta \sum_{\bx \in \M} \ELBO(\bx; \theta, \phi),
\end{align}
</script></div>
<p>rather than running BBVI’s <strong>Step 1</strong> as a subroutine. By leveraging the learnability of <script type="math/tex">\bx \mapsto \lambda^\ast</script>, this optimization procedure amortizes the cost of variational inference. If one further chooses to define <script type="math/tex">f_\phi</script> as a neural network, the result is the variational autoencoder.</p>

<h1 id="footnotes">Footnotes</h1>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The first equality only holds if the support of <script type="math/tex">q</script> includes that of <script type="math/tex">p</script>. If not, it is an inequality. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>



    </article>
    <span class="print-footer">Variational Autoencoders - Aditya Grover</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2018 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;ADITYA GROVER &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2018</span> 
</div>  
</footer>

  </body>
</html>
